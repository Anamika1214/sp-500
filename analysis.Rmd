---
title: "Group Project Report"
author: "Group D8"
date: "3/16/2020"
output: pdf_document
---

# Introduction:

In this project, we seek to inspect a well-known time series, the S&P 500 Index, in order to identify trends and attempt to make a future forecast.

## Abstract
Using one year of historical closing price values from the S&P 500 Index, we attempt to fit a variety of models and analyze the trends. From the portmanteu test on the original data, we could say that the data is not generated from a random noise process at the 5% level. To remove an upward trend, the data was differenced, we could no longer reject the null hypothesis that the series is a white noise process. 

TODO: Explain about all the models

Using cross validation, we identified the top ARIMA models to model this sequence using RMSE and AIC as metrics. The best model by this criteria is ARIMA(1, 1, 0). Using this model, we attempt to predict the next 5 closing price values and demonstrate the comparison to the actual values.

## Data
The S&P 500 Index is a collection of some of the largest US stocks from a variety of industries, weighted by their market capitalization. A closing price is the last price at which the stock was trading during the previous trading day. In the case of S&P 500, this is the last weighted calculation of the index for the trading day, calculated at 4PM ET/EDT. In this report, we use the S&P 500 index closing price for January 1, 2019 to Dec 31st 2019 for model training, and January 2 to January 8 2020 for model prediction. 

The data source is Yahoo Finance, which we used because it is a free, trusted source for accurate stock data. We used the quantmod package (Quantitative Financial Modelling & Trading Framework for R) which has an API for importing the data from Yahoo.

The S&P 500 is superior to individual stocks for modeling, as it is an aggregate -- this allows us to obtain a "big picture" in comparison to individual stocks, which are more volatile and subject to more extreme fluctuation. Our motivation for using this data was to obtain a prediction model based on historical price data. Such a model would not only be useful for profiting from market trades, but also as an indicator for US stock movement, and therefore GDP growth and economic sentiment. Of course, a simple model would be unlikely, so we used methods such as Neural Networks to model the data.  


# Analysis:

```{r setup, message = FALSE, warning = FALSE}
library(doSNOW)
library(forecast)
library(ggfortify)
library(gtools)
library(parallel)
library(quantmod)
library(tcltk)
options('getSymbols.warning4.0' = F)
```

We aim to predict the daily closing values of The S&P 500 Index from 2020–01–02 to 2020–01–08. In this case, we will use one year of S&P 500 historical closing price values from 2019–01–01 to 2019–12–31, which can be accessed through yahoo finance. After downloading, the dataset looks like this:


### Using data from Yahoo! Finance

```{r Load data}
getSymbols(Symbols = '^GSPC',
           src     = 'yahoo',
           auto.assign = T,
           from    = '2019-01-01',
           to      = '2020-01-01')
data <- GSPC[, 'GSPC.Close']
head(data)
tail(data)
sp500 <- ts(data)
```



## Time Series Plot

```{r time series plot}
ggtsdisplay(sp500, main = 'S&P 500 Index Closing Price')
```

The time series plot shows an increasing linear trend with no obvious seasonality.

ACF plot shows a very slow decay in time suggesting that the series might not be stationary.

PACF cuts off at lag 1



### Portmanteau Test

```{r}
Box.test(sp500, lag = 12, type = 'Box-Pierce')
```

The p-value of the portmanteau test for residuals is less than 0.05. Therefore, we have sufficient evidence to reject the null hypothesis that the series is white noise. 

Then, to make the series stationary, we compute the differences between consecutive observations.



## First Differenced Time Series Plot

```{r}
sp500 %>%
  diff() %>%
  ggtsdisplay(main = 'First Differenced S&P 500 Index Closing Price')
```

The differenced series plot shows variation without apparend trend.

Although both ACF and pacf plots show significant values at lag 7, 22,and 23, they drop to zero quickly and most of values are not significant. Based on that, we could probably conclude that the series is a white noise process.



### Portmanteau Test

```{r}
sp500 %>%
  diff() %>%
  Box.test(lag = 12, type = 'Box-Pierce')
```

The p-value of the portmanteau test for residuals is greater than 0.05. Therefore, we do not have sufficient evidence to reject the null hypothesis at 5% significant level, so we cannot conclude that the differenced series is not a white noise process.



# Modeling

In the next part, we will use four different forecasting techniques: Regression, Exponential Smoothing, Box-Jenkins Methodology, and Feedforward Neural Network.



## Regression (Linear model and Cubic Spline)

Since our time series data has a linear trend, we will try to model it using a trend variable.

```{r}
tslm_linear <- tslm(sp500 ~ trend)
summary(tslm_linear)
fcast_linear <- forecast(tslm_linear, h = 5)
```

### Forecast Plot

```{r}
autoplot(sp500) +
  autolayer(fitted(tslm_linear), series = 'Linear') +
  autolayer(fcast_linear, series = 'Linear') +
  xlab('Time') +
  ylab('Closing Stock Price') +
  ggtitle('S&P 500 Index Forecast') +
  guides(colour = guide_legend(title = ' '))
```

As we can see the series is not exactly linear so it might be better to use a non-linear regression like cubic spline. 

```{r}
t <- time(sp500)
t1 <- ts(pmax(0, t - 37))
t2 <- ts(pmax(0, t - 205))
tslm_spline <- tslm(sp500 ~ t + I(t^2) + I(t^3) + I(t1^3) + I(t2^3))
summary(tslm_spline)

fcast_spline <- forecast(tslm_spline,
                         newdata = data.frame(t = t[length(t)] + seq(5),
                                              t1 = t1[length(t1)] + seq(5),
                                              t2 = t2[length(t2)] + seq(5)))

```

### Forecast Plot

```{r}
autoplot(sp500) +
  autolayer(fitted(tslm_spline), series = 'Cubic Spline') +
  autolayer(fcast_spline, series = 'Cubic Spline') +
  xlab('Time') +
  ylab('Closing Stock Price') +
  ggtitle('S&P 500 Index Forecast') +
  guides(colour = guide_legend(title = ' '))
```

### Five-step Time Series Cross-Validation

We will not be doing CV for cubic spline because we would have to determine the location and number of knots for each fold.

```{r}
ff_linear <- function(x, h) {
  forecast(tslm(x ~ trend), h = h)
}
e_linear <- tsCV(sp500, ff_linear, h = 5)
rmse_linear <- sqrt(mean(e_linear^2, na.rm = T))
```



## Exponential Smoothing

### Simple Exponential Smoothing

We will use this method because it is suitable for forecasting data with no clear seasonal pattern.

```{r}
fcast_ses <- ses(sp500, h = 5, level = 95)
summary(fcast_ses)
```

### Forecast Plot

```{r}
autoplot(sp500) +
  autolayer(fitted(fcast_ses), series = 'SES') +
  autolayer(fcast_ses, series = 'SES') +
  xlab('Time') +
  ylab('Closing Stock Price') +
  ggtitle('S&P 500 Index Forecast') +
  guides(colour = guide_legend(title = ' '))
```



### Holt's linear trend method

```{r}
holt_fcast <- holt(sp500, h = 5, level = 95)
summary(holt_fcast)
```

### Forecast Plot

```{r}
autoplot(sp500) +
  autolayer(fitted(holt_fcast), series = "Holt's") +
  autolayer(holt_fcast, series = "Holt's") +
  xlab('Time') +
  ylab('Closing Stock Price') +
  ggtitle('S&P 500 Index Forecast') +
  guides(colour = guide_legend(title = ' '))
```
```{r}
holt_fcast_damped <- holt(sp500, h = 5, level = 95, damped = T)
summary(holt_fcast_damped)
```

### Forecast Plot

```{r}
autoplot(sp500) +
  autolayer(fitted(holt_fcast_damped), series = "Damped Holt's") +
  autolayer(holt_fcast_damped, series = "Damped Holt's") +
  xlab('Time') +
  ylab('Closing Stock Price') +
  ggtitle('S&P 500 Index Forecast') +
  guides(colour = guide_legend(title = ' '))
```


### Five-step Time Series Cross-Validation

```{r}
tsCV_ses <- tsCV(sp500, ses, h = 5)
rmse_ses <- sqrt(mean(tsCV_ses^2, na.rm = T))

tsCV_holt <- tsCV(sp500, holt, h = 5)
rmse_holts <- sqrt(mean(tsCV_holt^2, na.rm = T))

tsCV_holt_damped <- tsCV(sp500, holt, damped = T, h = 5)
rmse_damped_holts <- sqrt(mean(tsCV_holt_damped^2, na.rm = T))
```



## Box-Jenkins Methodology


### Five-step Time Series Cross-Validation for ARIMA Model Selection

We will use a five-step time series cv to measure the goodness of prediction for all possible models (ARIMA(p, 1, q) where 0 <= p <= 4 and 0 <= q <= 4).

  For every i = 1, ..., 251:
    a) Train the model on every point before i
    b) Compute the test error on the held out point i
    b) Average the test errors

To evaluate the effectiveness of our methods, we will use the root mean square error (RMSE) and Akaike information criterion (AIC) metrics. For both metrics, the lower the value, the better the prediction.

```{r}
cl <- makeSOCKcluster(detectCores() - 1)
registerDoSNOW(cl)

order <- permutations(5, 2, 0:4, repeats = T)
order <- lapply(1:nrow(order), function(i) order[i, ]) # Convert matrix to list
pb <- tkProgressBar(max = length(order))
opts <- list(progress = function(n) setTkProgressBar(pb, n))

score <- foreach(pq = order, .options.snow = opts, .packages = c('forecast')) %dopar% {
  p <- pq[1]
  q <- pq[2]
  aic <- Arima(sp500, order = c(p, 1, q))$aic
  farima <- function(x, h) {
    forecast(Arima(x, order = c(p, 1, q)), h = h)
  }
  e <- tsCV(sp500, farima, h = 5)
  rmspe <- sqrt(mean(e^2, na.rm = T))
  return(c(p, q, aic, rmspe))
}

close(pb)
stopCluster(cl)

result <- data.frame(Reduce(rbind, score), row.names = NULL)
colnames(result) <- c('p', 'q', 'AIC', 'RMSE')
print(result)
```


### Best ARIMA models based on RMSPE

```{r}
head(result[order(result$RMSE),])
```

### Best ARIMA models based on AIC

```{r}
head(result[order(result$AIC),])
```

Best model: ARIMA(1, 1, 0), because it has the lowest RMSPE, and an AIC value that is very close to the second lowest.


### Fit best ARIMA model on the full training set

```{r}
arima_model <- Arima(sp500, order = c(1, 1, 0), include.constant = T)
summary(arima_model)
rmse_arima <- min(result$RMSE)
```

### Forecast Plot

```{r}
fcast_arima <- forecast(arima_model, h = 5, level = 95)
autoplot(sp500) +
  autolayer(fitted(fcast_arima), series = 'ARIMA(1,1,0)') +
  autolayer(fcast_arima, series = 'ARIMA(1,1,0)') +
  xlab('Time') +
  ylab('Closing Stock Price') +
  ggtitle('S&P 500 Index Forecast') +
  guides(colour = guide_legend(title = ' '))
```

### Residual Analysis

```{r}
ggtsdiag(arima_model)
```

```{r}
checkresiduals(arima_model, lag = 12)
```

We can see that there is no pattern apparent in the residuals analysis plot. The acf values are not significant for lags other than 0. THe p-values for Ljung-Box test are also large suggesting nothing untoward about the fit of the model. Hence, we will use the ARIMA(1,1,0) to forcast the next 5 closing price of S&P 500, which are the closing values from 2020–01–02 to 2020–01–08.



## Feedforward Neural Network

We will use a five-step time series cv to measure the goodness of prediction for all possible models (NNAR(p,1) where 0 <= p <= 4).

  For every i = 1, ..., 251:
    a) Train the model on every point before i
    b) Compute the test error on the held out point i
    b) Average the test errors

To evaluate the effectiveness of our methods, we will use the root mean square error (RMSE) metric. For RMSE, the lower the value, the better the prediction.

```{r}
cl <- makeSOCKcluster(detectCores() - 1)
registerDoSNOW(cl)

order <- 1:4
pb <- tkProgressBar(max = length(order))
opts <- list(progress = function(n) setTkProgressBar(pb, n))

score <- foreach(p = order, .options.snow = opts, .packages = c('forecast')) %dopar% {
  fnnetar <- function(x, h) {
    set.seed(2020)
    forecast(nnetar(y = x, p = p, size = 1), h = h)
  }
  e <- tsCV(sp500, fnnetar, h = 5)
  rmspe <- sqrt(mean(e^2, na.rm = T))
  return(c(p, rmspe))
}

close(pb)
stopCluster(cl)

result <- data.frame(Reduce(rbind, score), row.names = NULL)
colnames(result) <- c('p', 'RMSE')
print(result)
```

Best model: NNAR(1,1), because it has the lowest RMSE.

### Fit the best NNAR model

```{r}
set.seed(2020)
nnar_model <- nnetar(y = sp500, p = 1, size = 1, repeats = 200)
print(nnar_model)
rmse_nnar <- min(result$RMSE)
```

### Forecast Plot

```{r}
fcast_nnar <- forecast(nnar_model, h = 5)
autoplot(sp500) +
  autolayer(fitted(fcast_nnar), series = 'NNAR(1,1)') +
  autolayer(fcast_nnar, series = 'NNAR(1,1)') +
  xlab('Time') +
  ylab('Closing Stock Price') +
  ggtitle('S&P 500 Index Forecast') +
  guides(colour = guide_legend(title = ' '))
```



# Forecasting


### CV RMSE Comparison

```{r}
rmse_cv <- setNames(data.frame(rmse_linear, rmse_ses, rmse_holts, rmse_damped_holts, rmse_arima, rmse_nnar, row.names = 'RMSE'),
                    c('Linear Regression', 'SES', "Holt's", "Damped Holt's", "ARIMA", 'NNAR'))
print(rmse_cv)
```

We can see that ARIMA has the lowest RMSE obtained from cross validation.
We will now compare the top 3 model using the test set. Since we could not calculate the CV RMSE for cubic spline, we will evaluate it with the test set too.


### Retrieve the next 5 closing prices (First 5 in 2020)

```{r}
getSymbols(Symbols = '^GSPC',
           src     = 'yahoo',
           auto.assign = T,
           from    = '2020-01-01',
           to      = '2020-03-31')
data <- GSPC[1:5, 'GSPC.Close']
head(data)
test <- as.vector(data)
```


### Evaluate MSE for Top 3 Models

```{r}
rmse_test <- data.frame(rbind(accuracy(fcast_ses$mean, test),
                              accuracy(fcast_arima$mean, test),
                              accuracy(fcast_nnar$mean, test),
                              accuracy(fcast_spline$mean, test)),
                        row.names = c('SES', 'ARIMA', 'NNAR', 'Cubic Spline'))
print(rmse_test[order(rmse_test$RMSE), ])
```

We can see that ARIMA has the lowest test RMSE score. Cubic Spline seems to have the worst performance here, this is not surprising as it can still be further improved by using higher of knots or we can even use smoothing splines instead so we would not need to optimize the number of knots.

```{r}
cbind(data.frame(fcast_arima), Actual = test)
```

Our best model ARIMA has a Root Mean Squared Error of 12.31737, and we also obseved all actual values are lying between the 95% prediction confidence interval. Thus, the ARIMA(1,1,0) seems to perform well in the prediction. 

### Forecast vs Actual

```{r}
ggplot(data.frame(data), aes(x = index(data))) +
  geom_line(aes(y = GSPC.Close, color = 'Actual')) +
  geom_line(aes(y = fcast_arima$mean, color = 'ARIMA')) +
  geom_line(aes(y = fcast_nnar$mean, color = 'NNAR')) +
  geom_line(aes(y = fcast_ses$mean, color = 'SES')) +
  xlab('Time') +
  ylab('Closing Stock Price') +
  ggtitle('S&P 500 Index Forecast vs Actual')
```

# Conclusions:

TODO: Include other models (Linear, Cubic Spline, SES, Holt's, Damped Holt's, Neural Net)

We set out to determine if there was a meaningful model that could be found, which would be able to predict future stock price of the S&P500 Index. In this project, we collected 252 observations from Yahoo Finance API. The data represents the daily stock price from 2019–01–01 to 2019–12–31 (not including 104 days for the weekends when the exchange is closed). We were also able to obtain the actual stock prices for the first five weekdays of 2020 which we will attempt to forecast using the data from 2019. The actual values compared to our forecasted values will be used as a measure of the quality of our model.

Preliminary data exploration showed that the daily stock price of the S&P500 index was not a stationary series, the data plot showed an obvious upward trend, without seasonality or periodicity. Moreover, the acf showed significant correlation at all lag points. Ljung-Box p-value was less than 0.05, therefore we rejected the null hypothesis and concluded that the series was not a realization of a white noise (stationary) process. Before further downstream analysis, we had to transform the original, non-stationary, data series into a stationary series. 

The series was transformed using diff = 1, generate a new series. When plotted, the new series data values showed no obvious trend. The acf and pacf plots show significant values at lag 7, 22, and 23. After doing the Ljung-Box test (p-value greater thn 0.05), we were unable to reject the null hypothesis and concluded that the new series was indeed a realization from a stationary process such that the significant values in the acf and pacf were likely to be outliers.

Next we wanted to find the optimal model for forecasting. To do this, we enumerated over possible values of p and q (d = 1), constrainted by p >= 0 and p <= 4, and q >= 0, and q <= 4. We used a five-step time series cross-validation approach to quantify the accuracy of each model. The chosen model used ARIMA(1, 1, 0), because it has the lowest RMSPE (35.82020), and AIC value (2277.979) was extremely close to the second lowest AIC (2277.018). Fitting the ARIMA(1,1,0) model to the full dataset returned an alpha coefficient of -0.0849. Residual analysis on the fitted model shows that the residuals follow a realization of a white noise process, as expected from a good fitting model.

We forecasted the first five stock prices from the year 2020. The actual values are shown to fall within the 95% confidence interval of the forecasted values. As the time increases into the future, the confidence interval widens, as expected. Overall, we conclude that an ARIMA(1,1,0) model is a good approximatation for the daily stock price of the S&P500 index for the year of 2019. The ARIMA(1,1,0) model is also accuracte for forecasting purposes, up until the first 5 weekdays of 2020.

During post-analysis, we looked at various years through out the S&P500 data and realized that we had been extremely fortunate selecting 2019 as our training dataset. The trends in other years were not as clear and there is no guarantee that using other years as training would yield an ARIMA(1,1,0) as the best fitted model. Furthermore, the stock market experiences various periods of extreme growth, stagnation, and crashes, however there was no such novel event during the year of 2019. Taking into account these extenuating circumstances, it is extremely unlikely that an ARIMA(1,1,0) would be the best fitted model for the S&P500 in general.

Moving forward, we would like to try to use the entire S&P500 dataset from inception in 1926 until 2019. This may yield a more accurate model of the S&P500 stock index. However, even that model is unlikely to be able to predict market crashes like what we are currently experiencing due to the COVID-19 global pandemic. More sophisticated models which can account for anomaly detection would be necessary in order to generate a model which can accurately model the S&P500 index. 

















